{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "from multiprocessing.dummy import Pool\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "import time\n",
    "PADDING = 50\n",
    "ready_to_detect_identity = True\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def say(text):\n",
    "    subprocess.call(['say', text])\n",
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.3):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database():\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(\"images/*\"):\n",
    "        file = ''.join([i for i in file if not i.isdigit()])\n",
    "        identity = os.path.splitext(os.path.basename(file))[0]\n",
    "        if identity in database:\n",
    "            database[identity] = database[identity].append(img_path_to_encoding(file, FRmodel))\n",
    "        else:\n",
    "            database[identity] = [img_path_to_encoding(file, FRmodel)]\n",
    "    return database\n",
    "\n",
    "def webcam_face_recognizer(database):\n",
    "    \"\"\"\n",
    "    Runs a loop that extracts images from the computer's webcam and determines whether or not\n",
    "    it contains the face of a person in our database.\n",
    "\n",
    "    If it contains a face, an audio message will be played welcoming the user.\n",
    "    If not, the program will process the next frame from the webcam\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "\n",
    "    cv2.namedWindow(\"preview\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    while vc.isOpened():\n",
    "        _, frame = vc.read()\n",
    "        img = frame\n",
    "\n",
    "        # We do not want to detect a new identity while the program is in the process of identifying another person\n",
    "        if ready_to_detect_identity:\n",
    "            img = process_frame(img, frame, face_cascade)   \n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"preview\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    cv2.destroyWindow(\"preview\")\n",
    "\n",
    "def process_frame(img, frame, face_cascade):\n",
    "    \"\"\"\n",
    "    Determine whether the current frame contains the faces of people from our database\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Loop through all the faces detected and determine whether or not they are in the database\n",
    "    identities = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        x1 = x-PADDING\n",
    "        y1 = y-PADDING\n",
    "        x2 = x+w+PADDING\n",
    "        y2 = y+h+PADDING\n",
    "\n",
    "        img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,0,0),2)\n",
    "\n",
    "        identity = find_identity(frame, x1, y1, x2, y2)\n",
    "\n",
    "        if identity is not None:\n",
    "            identities.append(identity)\n",
    "\n",
    "    if identities != []:\n",
    "        cv2.imwrite('example.png',img)\n",
    "\n",
    "        ready_to_detect_identity = False\n",
    "        pool = Pool(processes=1) \n",
    "        # We run this as a separate process so that the camera feedback does not freeze\n",
    "        pool.apply_async(welcome_users, [identities])\n",
    "    return img\n",
    "\n",
    "def find_identity(frame, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Determine whether the face contained within the bounding box exists in our database\n",
    "\n",
    "    x1,y1_____________\n",
    "    |                 |\n",
    "    |                 |\n",
    "    |_________________x2,y2\n",
    "\n",
    "    \"\"\"\n",
    "    height, width, channels = frame.shape\n",
    "    # The padding is necessary since the OpenCV face detector creates the bounding box around the face and not the head\n",
    "    part_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]\n",
    "    \n",
    "    return who_is_it(part_image, database, FRmodel)\n",
    "\n",
    "def who_is_it(image, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    encoding = img_to_encoding(image, model)\n",
    "    \n",
    "    min_dist = 100\n",
    "    identity = None\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_encs) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database.\n",
    "        dists = [np.linalg.norm(db_enc - encoding) for db_enc in db_encs]\n",
    "        dist = np.min(dists)\n",
    "        print('distance for %s is %s' %(name, dist))\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.52:\n",
    "        return None\n",
    "    else:\n",
    "        return str(identity)\n",
    "\n",
    "def welcome_users(identities):\n",
    "    \"\"\" Outputs a welcome audio message to the users \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "\n",
    "    if len(identities) == 1:\n",
    "        welcome_message = 'I see one face. Hello, %s.' % identities\n",
    "    else:\n",
    "        welcome_message = 'I see ' + str(len(identities)) + ' faces. '\n",
    "        for identity_id in range(len(identities)-1):\n",
    "            welcome_message += 'I see %s, ' % identities[identity_id]\n",
    "        welcome_message += 'and I see %s.' % identities[-1]\n",
    "        say(welcome_message)\n",
    "        print(welcome_message)\n",
    "\n",
    "    # Allow the program to start detecting identities again\n",
    "    time.sleep(5)\n",
    "    ready_to_detect_identity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1296c1f0ecaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7b401906ff7f>\u001b[0m in \u001b[0;36mprepare_database\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdatabase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdatabase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_path_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/FaceNet/fr_utils.py\u001b[0m in \u001b[0;36mimg_path_to_encoding\u001b[0;34m(image_path, model)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_path_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/FaceNet/fr_utils.py\u001b[0m in \u001b[0;36mimg_to_encoding\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "database = prepare_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "\n",
    "for file in glob.glob(\"images/*\"):\n",
    "    file = ''.join([i for i in file if not i.isdigit()])\n",
    "    identity = os.path.splitext(os.path.basename(file))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/Subrot.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3cc9b661bae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_path_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/FaceNet/fr_utils.py\u001b[0m in \u001b[0;36mimg_path_to_encoding\u001b[0;34m(image_path, model)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_path_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/FaceNet/fr_utils.py\u001b[0m in \u001b[0;36mimg_to_encoding\u001b[0;34m(image, model)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "img_path_to_encoding(file, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/Subrot24.png\n",
      "images/Eric48.png\n",
      "images/Eric60.png\n",
      "images/Subrot18.png\n",
      "images/Nico58.png\n",
      "images/Nico59.png\n",
      "images/Nico71.png\n",
      "images/Eric75.png\n",
      "images/Eric61.png\n",
      "images/Subrot31.png\n",
      "images/Eric49.png\n",
      "images/Subrot25.png\n",
      "images/Subrot8.png\n",
      "images/Subrot27.png\n",
      "images/Eric77.png\n",
      "images/Eric63.png\n",
      "images/Eric88.png\n",
      "images/Nico73.png\n",
      "images/Nico67.png\n",
      "images/Nico98.png\n",
      "images/Nico66.png\n",
      "images/Nico72.png\n",
      "images/Eric89.png\n",
      "images/Eric62.png\n",
      "images/Subrot26.png\n",
      "images/Subrot32.png\n",
      "images/Subrot9.png\n",
      "images/Eric72.png\n",
      "images/Eric66.png\n",
      "images/Subrot36.png\n",
      "images/Subrot22.png\n",
      "images/Nico62.png\n",
      "images/Nico89.png\n",
      "images/Nico88.png\n",
      "images/Nico63.png\n",
      "images/Nico77.png\n",
      "images/Eric98.png\n",
      "images/Subrot23.png\n",
      "images/Subrot37.png\n",
      "images/Eric67.png\n",
      "images/Eric73.png\n",
      "images/Eric65.png\n",
      "images/Eric71.png\n",
      "images/Subrot21.png\n",
      "images/Eric59.png\n",
      "images/Subrot35.png\n",
      "images/Nico49.png\n",
      "images/Nico61.png\n",
      "images/Nico75.png\n",
      "images/Nico74.png\n",
      "images/Nico60.png\n",
      "images/Subrot34.png\n",
      "images/Eric58.png\n",
      "images/Eric70.png\n",
      "images/Eric64.png\n",
      "images/Subrot47.png\n",
      "images/Subrot53.png\n",
      "images/Eric17.png\n",
      "images/Subrot84.png\n",
      "images/Nico13.png\n",
      "images/Nico12.png\n",
      "images/Nico8.png\n",
      "images/Subrot91.png\n",
      "images/Subrot85.png\n",
      "images/Eric16.png\n",
      "images/Subrot52.png\n",
      "images/Subrot46.png\n",
      "images/Eric8.png\n",
      "images/Eric28.png\n",
      "images/Subrot44.png\n",
      "images/Subrot78.png\n",
      "images/Subrot87.png\n",
      "images/Nico10.png\n",
      "images/Nico38.png\n",
      "images/Nico39.png\n",
      "images/Nico11.png\n",
      "images/Subrot86.png\n",
      "images/Subrot92.png\n",
      "images/Subrot79.png\n",
      "images/Eric15.png\n",
      "images/Subrot45.png\n",
      "images/Eric29.png\n",
      "images/Subrot51.png\n",
      "images/Eric9.png\n",
      "images/Eric11.png\n",
      "images/Subrot69.png\n",
      "images/Subrot55.png\n",
      "images/Subrot41.png\n",
      "images/Subrot96.png\n",
      "images/Subrot82.png\n",
      "images/Nico29.png\n",
      "images/Nico15.png\n",
      "images/Nico28.png\n",
      "images/Subrot83.png\n",
      "images/Subrot97.png\n",
      "images/Subrot40.png\n",
      "images/Eric38.png\n",
      "images/Subrot54.png\n",
      "images/Subrot68.png\n",
      "images/Eric10.png\n",
      "images/Eric12.png\n",
      "images/Subrot42.png\n",
      "images/Subrot56.png\n",
      "images/Subrot81.png\n",
      "images/Subrot95.png\n",
      "images/Nico16.png\n",
      "images/Nico17.png\n",
      "images/Subrot80.png\n",
      "images/Subrot57.png\n",
      "images/Subrot43.png\n",
      "images/Eric13.png\n",
      "images/Eric2.png\n",
      "images/Subrot66.png\n",
      "images/Subrot72.png\n",
      "images/Eric22.png\n",
      "images/Eric36.png\n",
      "images/Nico0.png\n",
      "images/Nico32.png\n",
      "images/Nico33.png\n",
      "images/Nico27.png\n",
      "images/Nico1.png\n",
      "images/Subrot98.png\n",
      "images/Eric37.png\n",
      "images/Eric23.png\n",
      "images/Subrot73.png\n",
      "images/Subrot67.png\n",
      "images/Eric3.png\n",
      "images/Eric1.png\n",
      "images/Subrot71.png\n",
      "images/Subrot65.png\n",
      "images/Eric35.png\n",
      "images/Eric21.png\n",
      "images/Subrot59.png\n",
      "images/Nico31.png\n",
      "images/Nico25.png\n",
      "images/Nico18.png\n",
      "images/Nico24.png\n",
      "images/Nico30.png\n",
      "images/Subrot58.png\n",
      "images/Eric20.png\n",
      "images/Subrot64.png\n",
      "images/Subrot70.png\n",
      "images/Eric0.png\n",
      "images/Subrot48.png\n",
      "images/Eric30.png\n",
      "images/Eric24.png\n",
      "images/Subrot74.png\n",
      "images/Subrot60.png\n",
      "images/Eric18.png\n",
      "images/Nico6.png\n",
      "images/Nico34.png\n",
      "images/Nico20.png\n",
      "images/Nico21.png\n",
      "images/Nico35.png\n",
      "images/Nico7.png\n",
      "images/Subrot61.png\n",
      "images/Subrot75.png\n",
      "images/Eric25.png\n",
      "images/Eric31.png\n",
      "images/Subrot49.png\n",
      "images/Eric5.png\n",
      "images/Eric27.png\n",
      "images/Eric33.png\n",
      "images/Subrot63.png\n",
      "images/Subrot77.png\n",
      "images/Subrot88.png\n",
      "images/Nico5.png\n",
      "images/Nico37.png\n",
      "images/Nico36.png\n",
      "images/Nico4.png\n",
      "images/Subrot89.png\n",
      "images/Subrot76.png\n",
      "images/Subrot62.png\n",
      "images/Eric32.png\n",
      "images/Eric26.png\n",
      "images/Subrot11.png\n",
      "images/Eric69.png\n",
      "images/Subrot39.png\n",
      "images/Subrot2.png\n",
      "images/Eric41.png\n",
      "images/Eric55.png\n",
      "images/Eric96.png\n",
      "images/Nico45.png\n",
      "images/Nico86.png\n",
      "images/Nico92.png\n",
      "images/Nico93.png\n",
      "images/Nico87.png\n",
      "images/Nico78.png\n",
      "images/Eric97.png\n",
      "images/Eric83.png\n",
      "images/Eric54.png\n",
      "images/Eric40.png\n",
      "images/Subrot38.png\n",
      "images/Eric68.png\n",
      "images/Subrot10.png\n",
      "images/Eric42.png\n",
      "images/Subrot1.png\n",
      "images/Eric95.png\n",
      "images/Eric81.png\n",
      "images/Nico52.png\n",
      "images/Nico46.png\n",
      "images/Nico91.png\n",
      "images/Nico85.png\n",
      "images/Nico84.png\n",
      "images/Nico90.png\n",
      "images/Nico47.png\n",
      "images/Eric80.png\n",
      "images/Eric94.png\n",
      "images/Subrot0.png\n",
      "images/Subrot13.png\n",
      "images/Eric47.png\n",
      "images/Subrot4.png\n",
      "images/Subrot17.png\n",
      "images/Eric90.png\n",
      "images/Eric84.png\n",
      "images/Nico43.png\n",
      "images/Nico94.png\n",
      "images/Nico80.png\n",
      "images/Nico81.png\n",
      "images/Nico95.png\n",
      "images/Nico42.png\n",
      "images/Nico56.png\n",
      "images/Eric91.png\n",
      "images/Subrot16.png\n",
      "images/Subrot5.png\n",
      "images/Eric46.png\n",
      "images/Subrot7.png\n",
      "images/Eric44.png\n",
      "images/Eric50.png\n",
      "images/Subrot28.png\n",
      "images/Eric78.png\n",
      "images/Subrot14.png\n",
      "images/Eric87.png\n",
      "images/Nico68.png\n",
      "images/Nico40.png\n",
      "images/Nico54.png\n",
      "images/Nico97.png\n",
      "images/Nico96.png\n",
      "images/Nico55.png\n",
      "images/Nico41.png\n",
      "images/Eric86.png\n",
      "images/Subrot15.png\n",
      "images/Eric79.png\n",
      "images/Subrot29.png\n",
      "images/Eric51.png\n",
      "images/Subrot6.png\n"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "\n",
    "# load all the images of individuals to recognize into the database\n",
    "for file_num in glob.glob(\"images/*\"):\n",
    "    print(file_num)\n",
    "    identity = os.path.splitext(os.path.basename(file_num))[0]\n",
    "    identity = ''.join([i for i in identity if not i.isdigit()])\n",
    "    if identity in database:\n",
    "        database[identity] = database[identity] + [img_path_to_encoding(file_num, FRmodel)]\n",
    "    else:\n",
    "        database[identity] = [img_path_to_encoding(file_num, FRmodel)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nums = glob.glob(\"images/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = file_nums[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = os.path.splitext(os.path.basename(file_num))[0]\n",
    "identity = ''.join([i for i in identity if not i.isdigit()])\n",
    "database[identity] = [img_path_to_encoding(file_num, FRmodel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00502624,  0.11563899,  0.10302966,  0.08055655,  0.04786522,\n",
       "        0.09058852, -0.01047758, -0.21145758,  0.07824475, -0.08698074,\n",
       "       -0.00376674,  0.04527403, -0.0492423 ,  0.07970748, -0.0373813 ,\n",
       "       -0.03539709,  0.03340322,  0.02649122, -0.09057584,  0.01418769,\n",
       "       -0.00414894,  0.0757481 , -0.13213   ,  0.10599813, -0.00400234,\n",
       "       -0.03750094, -0.1374872 ,  0.0836149 ,  0.05501855,  0.10377266,\n",
       "        0.01811191,  0.16998237, -0.05788425,  0.00348586,  0.03582006,\n",
       "        0.11841219, -0.00762442, -0.0467183 , -0.02028409, -0.02277073,\n",
       "        0.04087079,  0.02053821, -0.04727909, -0.22694369,  0.11160266,\n",
       "        0.14258184,  0.00825016,  0.01483681, -0.10642183, -0.11865205,\n",
       "       -0.03519388,  0.13819292,  0.14502957, -0.05833407,  0.04039501,\n",
       "        0.06369919, -0.09414969,  0.06669276, -0.10183578,  0.04110904,\n",
       "       -0.01602299,  0.18485168,  0.0267094 , -0.08616544,  0.00476379,\n",
       "        0.135763  ,  0.08840923,  0.02333755, -0.15997659,  0.1159075 ,\n",
       "        0.07250818, -0.04376869, -0.01605072, -0.00101575,  0.05864776,\n",
       "        0.12463808,  0.00537555, -0.06869645, -0.01685989, -0.1019958 ,\n",
       "        0.04720696,  0.20806769, -0.05203115, -0.04750498,  0.02684164,\n",
       "        0.01510856, -0.03093105, -0.0208824 , -0.1549884 ,  0.15627864,\n",
       "        0.09792003, -0.04850574,  0.09722414,  0.17974176, -0.03153401,\n",
       "        0.05034753,  0.00588666, -0.1246604 ,  0.12262145,  0.0133022 ,\n",
       "        0.11449846,  0.01139978, -0.14030635,  0.00115571, -0.08569442,\n",
       "        0.13833417,  0.020535  , -0.10721137, -0.07073175,  0.03807883,\n",
       "        0.05251649, -0.00582946, -0.02000251, -0.04165875,  0.01148286,\n",
       "       -0.00218983, -0.21946189,  0.04572943, -0.01566153,  0.01657498,\n",
       "        0.01156213, -0.00665634, -0.00414961, -0.06251649,  0.03067994,\n",
       "        0.24082738,  0.01388597, -0.15369752], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database['Subrot'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faced'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3559e8a6214b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfaced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaceDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfaced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotate_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faced'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "from faced.detector import FaceDetector\n",
    "from faced.utils import annotate_image\n",
    "\n",
    "face_detector = FaceDetector()\n",
    "\n",
    "img = cv2.imread('test.png')\n",
    "rgb_img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "thresh = 0.9\n",
    "\n",
    "# Receives RGB numpy image (HxWxC) and\n",
    "# returns (x_center, y_center, width, height, prob) tuples. \n",
    "bboxes = face_detector.predict(rgb_img, thresh)\n",
    "\n",
    "def crop(img,bbox):\n",
    "    x, y, w, h, p = bbox\n",
    "    x1 = int(x - w/2)\n",
    "    y1 = int(y - h/2)\n",
    "    x2 = int(x + w/2)\n",
    "    y2 = int(y + h/2)\n",
    "    height, width, channels = img.shape\n",
    "    return img[max(0, y1):min(height, y2), max(0, x1):min(width, x2),:]\n",
    "\n",
    "test_img = crop(rgb_img,bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.7.0\n",
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.6\n",
      "anaconda-project==0.8.2\n",
      "appdirs==1.4.3\n",
      "appnope==0.1.0\n",
      "appscript==1.0.1\n",
      "asn1crypto==0.24.0\n",
      "astor==0.7.1\n",
      "astroid==2.1.0\n",
      "atomicwrites==1.3.0\n",
      "attrs==18.2.0\n",
      "autograd==1.2\n",
      "Automat==0.7.0\n",
      "Babel==2.6.0\n",
      "backcall==0.1.0\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "beautifulsoup4==4.7.1\n",
      "bitarray==0.8.3\n",
      "bleach==3.1.0\n",
      "boto==2.49.0\n",
      "certifi==2018.11.29\n",
      "cffi==1.12.1\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==0.7.0\n",
      "clyent==1.2.2\n",
      "colorama==0.4.1\n",
      "conda==4.6.7\n",
      "conda-build==3.15.1\n",
      "conda-verify==3.1.1\n",
      "constantly==15.1.0\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.5\n",
      "cycler==0.10.0\n",
      "Cython==0.29.5\n",
      "cytoolz==0.9.0.1\n",
      "dash==0.27.0\n",
      "dash-core-components==0.30.2\n",
      "dash-html-components==0.13.2\n",
      "dash-renderer==0.14.1\n",
      "dask==1.1.1\n",
      "decorator==4.3.2\n",
      "defusedxml==0.5.0\n",
      "distributed==1.25.3\n",
      "docutils==0.14\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.0.2\n",
      "filelock==3.0.10\n",
      "Flask==1.0.2\n",
      "Flask-Compress==1.4.0\n",
      "Flask-Cors==3.0.7\n",
      "future==0.17.1\n",
      "gast==0.2.1.post0\n",
      "gevent==1.3.7\n",
      "glob2==0.6\n",
      "gmpy2==2.0.8\n",
      "greenlet==0.4.13\n",
      "grpcio==1.16.1\n",
      "h5py==2.9.0\n",
      "heapdict==1.0.0\n",
      "html5lib==1.0.1\n",
      "hyperlink==17.3.1\n",
      "idna==2.8\n",
      "imagesize==1.1.0\n",
      "importlib-metadata==0.0.0\n",
      "incremental==17.5.0\n",
      "ipykernel==5.1.0\n",
      "ipython==7.3.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "isort==4.3.4\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4\n",
      "jedi==0.13.2\n",
      "Jinja2==2.10\n",
      "jsonschema==3.0.0a3\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.4\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.35.4\n",
      "jupyterlab-launcher==0.13.1\n",
      "jupyterlab-server==0.2.0\n",
      "Keras==2.2.4\n",
      "Keras-Applications==1.0.7\n",
      "Keras-Preprocessing==1.0.9\n",
      "keyring==18.0.0\n",
      "kiwisolver==1.0.1\n",
      "lazy-object-proxy==1.3.1\n",
      "llvmlite==0.26.0\n",
      "locket==0.2.0\n",
      "lxml==4.3.1\n",
      "Markdown==2.6.11\n",
      "MarkupSafe==1.1.0\n",
      "matplotlib==3.0.2\n",
      "mccabe==0.6.1\n",
      "mistune==0.8.4\n",
      "more-itertools==4.3.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.6.1\n",
      "multipledispatch==0.6.0\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.4.1\n",
      "nbformat==4.4.0\n",
      "networkx==2.2\n",
      "nltk==3.2.4\n",
      "nose==1.3.7\n",
      "notebook==5.7.4\n",
      "nteract-on-jupyter==1.9.12\n",
      "numpy==1.14.2\n",
      "numpydoc==0.8.0\n",
      "olefile==0.46\n",
      "opencv-python==4.0.0.21\n",
      "openpyxl==2.6.0\n",
      "packaging==19.0\n",
      "pandas==0.24.1\n",
      "pandocfilters==1.4.2\n",
      "parso==0.3.4\n",
      "partd==0.3.9\n",
      "path.py==11.5.0\n",
      "pathlib2==2.3.3\n",
      "pep8==1.7.1\n",
      "pexpect==4.6.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==5.4.1\n",
      "pkginfo==1.5.0.1\n",
      "plotly==3.6.1\n",
      "pluggy==0.8.1\n",
      "ply==3.11\n",
      "prometheus-client==0.6.0\n",
      "prompt-toolkit==2.0.9\n",
      "protobuf==3.6.0\n",
      "psutil==5.5.1\n",
      "ptyprocess==0.6.0\n",
      "py==1.7.0\n",
      "pyasn1==0.4.4\n",
      "pyasn1-modules==0.2.4\n",
      "pycodestyle==2.5.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.2\n",
      "pyflakes==2.1.0\n",
      "Pygments==2.3.1\n",
      "PyHamcrest==1.9.0\n",
      "pykqml==1.0\n",
      "pylint==2.2.2\n",
      "pyodbc==4.0.25\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.3.1\n",
      "PyQt5==5.12\n",
      "PyQt5-sip==4.19.14\n",
      "pyrsistent==0.14.10\n",
      "PySocks==1.6.8\n",
      "pytest==4.3.0\n",
      "pytest-openfiles==0.3.1\n",
      "pytest-remotedata==0.3.1\n",
      "python-dateutil==2.8.0\n",
      "pytz==2018.9\n",
      "PyYAML==3.13\n",
      "pyzmq==18.0.0\n",
      "QtAwesome==0.5.6\n",
      "qtconsole==4.4.3\n",
      "QtPy==1.6.0\n",
      "requests==2.21.0\n",
      "retrying==1.3.3\n",
      "rope==0.10.7\n",
      "ruamel-yaml==0.15.71\n",
      "scikit-learn==0.20.2\n",
      "scipy==1.2.1\n",
      "Send2Trash==1.5.0\n",
      "service-identity==17.0.0\n",
      "simplegeneric==0.8.1\n",
      "simplekml==1.3.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.12.0\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==1.1.2\n",
      "sortedcontainers==2.1.0\n",
      "soupsieve==1.8\n",
      "Sphinx==1.8.4\n",
      "sphinxcontrib-websupport==1.1.0\n",
      "spyder==3.3.3\n",
      "spyder-kernels==0.4.2\n",
      "SQLAlchemy==1.2.18\n",
      "sympy==1.3\n",
      "tblib==1.3.2\n",
      "tensorboard==1.10.0\n",
      "tensorflow==1.10.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.4.2\n",
      "toolz==0.9.0\n",
      "tornado==5.1.1\n",
      "tqdm==4.31.1\n",
      "traitlets==4.3.2\n",
      "Twisted==17.5.0\n",
      "typed-ast==1.3.1\n",
      "typing==3.6.4\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.24.1\n",
      "virtualenv==16.4.1\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.4.2\n",
      "wrapt==1.11.1\n",
      "wurlitzer==1.0.2\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.1.4\n",
      "xlwings==0.15.2\n",
      "xlwt==1.3.0\n",
      "zict==0.1.3\n",
      "zipp==0.3.3\n",
      "zope.interface==4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
